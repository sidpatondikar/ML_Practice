{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzXMfgat4lN02gchNdQlhC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sidpatondikar/ML_Practice/blob/main/Unsupervised_Algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Means Clustering"
      ],
      "metadata": {
        "id": "AKiw1dR90S9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means Clustering is a popular unsupervised machine learning algorithm used for partitioning a dataset into clusters or groups of data points that are similar to each other. It is a centroid-based clustering algorithm, meaning that it assigns data points to clusters based on their proximity to cluster centroids. Here's how K-Means works:\n",
        "\n",
        "1. **Initialization**:\n",
        "   - The algorithm starts by randomly selecting K initial cluster centroids. These centroids can be randomly chosen data points or other initialization methods like K-Means++.\n",
        "\n",
        "2. **Assignment Step**:\n",
        "   - Each data point is assigned to the nearest centroid, creating K clusters. The assignment is based on a distance metric, typically Euclidean distance.\n",
        "\n",
        "3. **Update Step**:\n",
        "   - After assigning all data points to clusters, the centroids of the clusters are recalculated by taking the mean (average) of all data points in each cluster. These new centroids represent the \"center\" of their respective clusters.\n",
        "\n",
        "4. **Repeat**:\n",
        "   - Steps 2 and 3 are repeated iteratively until a stopping criterion is met. Common stopping criteria include a maximum number of iterations or when the centroids no longer change significantly between iterations.\n",
        "\n",
        "5. **Final Clusters**:\n",
        "   - When the algorithm converges (i.e., centroids no longer change or the maximum number of iterations is reached), it produces K clusters, and each data point belongs to one of these clusters.\n",
        "\n",
        "The key challenge in K-Means is to determine the optimal number of clusters, K. Two common methods for finding the optimal K are the Elbow Method and Silhouette Analysis:\n",
        "\n",
        "**Elbow Method**:\n",
        "- The Elbow Method is a heuristic technique to find the optimal number of clusters by evaluating the within-cluster sum of squares (WCSS) for different values of K.\n",
        "- It involves running K-Means for a range of K values and calculating the WCSS for each K.\n",
        "- The WCSS measures the total squared distance between data points and their assigned cluster centroids. A smaller WCSS indicates that the data points are closer to their centroids, suggesting better clustering.\n",
        "- The \"elbow point\" in the WCSS vs. K plot is where the rate of decrease in WCSS sharply changes. This point is often considered the optimal K.\n",
        "\n",
        "**Silhouette Analysis**:\n",
        "- Silhouette Analysis is another method to assess the quality of clustering for different values of K.\n",
        "- It measures how similar each data point is to its own cluster (cohesion) compared to other clusters (separation).\n",
        "- For each data point, a silhouette score is calculated, which ranges from -1 (poor clustering) to +1 (well-clustered).\n",
        "- The average silhouette score for a range of K values is computed, and the K that maximizes this score is considered the optimal number of clusters.\n",
        "\n",
        "In summary, K-Means is a centroid-based clustering algorithm that assigns data points to clusters based on their proximity to cluster centroids. To find the optimal number of clusters, the Elbow Method and Silhouette Analysis are commonly used techniques, each with its own advantages and limitations. These methods help determine the K value that produces the most meaningful and well-separated clusters for a given dataset."
      ],
      "metadata": {
        "id": "jbszb_yM0WWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchical Clustering"
      ],
      "metadata": {
        "id": "DPT5M93p0uEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hierarchical Clustering is an unsupervised machine learning technique used to group similar data points into clusters that form a hierarchical structure, often visualized as a tree-like diagram called a dendrogram. It works by iteratively merging or splitting clusters based on the similarity between data points. Here's how it works:\n",
        "\n",
        "**Agglomerative Hierarchical Clustering**:\n",
        "\n",
        "1. **Initialization**:\n",
        "   - Each data point is treated as a single cluster, so initially, there are as many clusters as there are data points.\n",
        "\n",
        "2. **Agglomeration**:\n",
        "   - At each step, the two closest clusters are merged into a single cluster. The distance between clusters can be measured using various linkage methods, which are explained later.\n",
        "\n",
        "3. **Dendrogram Construction**:\n",
        "   - As clusters are merged, a dendrogram is constructed. The height at which two clusters are merged in the dendrogram represents the distance (dissimilarity) at which they were combined.\n",
        "\n",
        "4. **Stopping Criterion**:\n",
        "   - The merging process continues until there is only one cluster left, containing all data points, or until a specific stopping criterion is met.\n",
        "\n",
        "5. **Cluster Extraction**:\n",
        "   - The optimal number of clusters is determined either by specifying a desired number or by cutting the dendrogram at a certain height to obtain the desired number of clusters.\n",
        "\n",
        "Finding the optimal number of clusters in hierarchical clustering can be done using the dendrogram:\n",
        "\n",
        "**Dendrogram Analysis**:\n",
        "\n",
        "1. **Visual Inspection**:\n",
        "   - Examine the dendrogram visually. The vertical lines (leaves) represent data points, and branches represent clusters.\n",
        "   - Look for the level of the dendrogram where clusters start to merge. This level can give insights into the optimal number of clusters.\n",
        "\n",
        "2. **Height Threshold**:\n",
        "   - Determine a height threshold on the dendrogram and cut it horizontally. The number of resulting clusters below the threshold is your desired number of clusters.\n",
        "\n",
        "3. **Elbow Method**:\n",
        "   - Similar to K-Means, you can use the Elbow Method with the total within-cluster variance (WCSS) at different levels of the dendrogram to find an \"elbow point\" that suggests the optimal number of clusters.\n",
        "\n",
        "Different Types of Linkages in Hierarchical Clustering:\n",
        "\n",
        "Linkage methods determine how the distance between clusters is calculated when deciding which clusters to merge. There are several types:\n",
        "\n",
        "1. **Single Linkage (Minimum Linkage)**:\n",
        "   - It measures the shortest distance between any two data points in different clusters.\n",
        "\n",
        "2. **Complete Linkage (Maximum Linkage)**:\n",
        "   - It measures the longest distance between any two data points in different clusters.\n",
        "\n",
        "3. **Average Linkage**:\n",
        "   - It calculates the average distance between all pairs of data points in different clusters.\n",
        "\n",
        "4. **Centroid Linkage**:\n",
        "   - It measures the distance between the centroids (means) of two clusters.\n",
        "\n",
        "5. **Ward Linkage**:\n",
        "   - It aims to minimize the increase in the total within-cluster variance when merging clusters.\n",
        "\n",
        "Each linkage method has its own characteristics, and the choice depends on the nature of the data and the problem at hand. Ward linkage is often used for its ability to produce compact, well-separated clusters, but the choice of linkage method should be guided by the specific goals of the clustering analysis."
      ],
      "metadata": {
        "id": "XCvS4W2u1A2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing (NLP)"
      ],
      "metadata": {
        "id": "oGQtJiGLr5wB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP stands for Natural Language Processing, which is a branch of artificial intelligence that focuses on enabling computers to understand, interpret, and generate human language in a way that's both meaningful and useful. NLP allows computers to interact with humans using natural language, like how we talk and write, making it a key technology behind chatbots, language translation, sentiment analysis, and more.\n",
        "\n",
        "Let's break down the text preprocessing steps, as well as tokenization, stemming, and lemmatization, with easy-to-understand explanations and code examples:\n",
        "\n",
        "### Text Preprocessing Steps:\n",
        "\n",
        "Text preprocessing involves cleaning and preparing the raw text data before it's fed into an NLP model. Here are the key steps:\n",
        "\n",
        "1. **Lowercasing**: Convert all text to lowercase to ensure consistent processing.\n",
        "2. **Punctuation Removal**: Remove punctuation marks like periods, commas, and exclamation marks.\n",
        "3. **Special Character Removal**: Remove special characters that don't carry significant meaning, such as emojis or symbols.\n",
        "4. **Whitespace Trimming**: Remove any unnecessary leading or trailing whitespace.\n",
        "5. **Stopword Removal**: Remove common stopwords (like \"and,\" \"the,\" \"is\") that do not contribute much to the text's meaning.\n",
        "6. **Spelling Correction (Optional)**: Correct common spelling errors using libraries like `pyspellchecker`.\n",
        "\n",
        "Here's a code example using Python and the `nltk` library for text preprocessing:\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation and special characters\n",
        "    text = ''.join([char for char in text if char.isalnum() or char.isspace()])\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "raw_text = \"Hello! This is an example text with punctuation, emojis ðŸ˜Š, and some stopwords.\"\n",
        "cleaned_text = preprocess_text(raw_text)\n",
        "print(cleaned_text)\n",
        "\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "\n",
        "```arduino\n",
        "\n",
        "hello example text punctuation emojis stopwords\n",
        "\n",
        "```\n",
        "\n",
        "### Tokenization:\n",
        "\n",
        "Tokenization is the process of breaking down a text into individual words or subwords, referred to as tokens. Tokens are the basic building blocks used for further analysis.\n",
        "\n",
        "Here's a code example of tokenization using Python and the `nltk` library:\n",
        "\n",
        "```python\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Tokenization is an important NLP concept.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "\n",
        "```css\n",
        "['Tokenization', 'is', 'an', 'important', 'NLP', 'concept', '.']\n",
        "```\n",
        "\n",
        "### Stemming and Lemmatization:\n",
        "\n",
        "Stemming and lemmatization are techniques used to reduce words to their base or root form. This helps in reducing inflected words to a common form for analysis.\n",
        "\n",
        "**Stemming** involves cutting off prefixes or suffixes to get to the base form of a word. It's a simpler and faster technique but may not always produce a valid word.\n",
        "\n",
        "**Lemmatization** is a more sophisticated technique that uses a vocabulary and morphological analysis to reduce words to their base form (lemma), ensuring that the resulting word is valid.\n",
        "\n",
        "Here's a code example using Python and the `nltk` library for both stemming and lemmatization:\n",
        "\n",
        "```python\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "word = \"running\"\n",
        "stemmed_word = stemmer.stem(word)\n",
        "lemmatized_word = lemmatizer.lemmatize(word, pos='v')\n",
        "\n",
        "print(\"Original Word:\", word)\n",
        "print(\"Stemmed Word:\", stemmed_word)\n",
        "print(\"Lemmatized Word:\", lemmatized_word)\n",
        "\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "\n",
        "```arduino\n",
        "Original Word: running\n",
        "Stemmed Word: run\n",
        "Lemmatized Word: run\n",
        "```\n",
        "\n",
        "Remember that stemming might not always produce valid words, whereas lemmatization ensures valid words but might be computationally more intensive.\n",
        "\n",
        "By following these preprocessing steps and understanding tokenization, stemming, and lemmatization, you're well on your way to preparing text data for effective NLP tasks!\n",
        "\n",
        "# Part of Speech Tagging and NER\n",
        "\n",
        "Certainly! Let's dive into Part-of-Speech (POS) tagging and entity tagging, along with explanations and code examples.\n",
        "\n",
        "### **Part-of-Speech (POS) Tagging:**\n",
        "\n",
        "Part-of-Speech tagging is the process of assigning grammatical categories (such as noun, verb, adjective, etc.) to each word in a sentence. This helps in understanding the syntactic structure of a sentence.\n",
        "\n",
        "Here's a code example of POS tagging using Python and the **`nltk`** library:\n",
        "\n",
        "```python\n",
        "pythonCopy code\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sentence = \"The cat is sitting on the mat.\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "print(pos_tags)\n",
        "\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "\n",
        "```css\n",
        "\n",
        "[('The', 'DT'), ('cat', 'NN'), ('is', 'VBZ'), ('sitting', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('mat', 'NN'), ('.', '.')]\n",
        "\n",
        "```\n",
        "\n",
        "In the output, each word is paired with its corresponding POS tag.\n",
        "\n",
        "### **Entity Tagging (Named Entity Recognition):**\n",
        "\n",
        "Entity tagging, also known as Named Entity Recognition (NER), involves identifying and classifying named entities in text, such as names of people, places, organizations, dates, and more.\n",
        "\n",
        "Here's a code example of entity tagging using Python and the **`nltk`** library:\n",
        "\n",
        "```python\n",
        "pythonCopy code\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "sentence = \"Apple Inc. was founded by Steve Jobs in Cupertino, California.\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "ner_tags = nltk.ne_chunk(pos_tags)\n",
        "\n",
        "print(ner_tags)\n",
        "\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "\n",
        "```scss\n",
        "\n",
        "(S\n",
        "  (ORGANIZATION Apple/NNP Inc./NNP)\n",
        "  was/VBD\n",
        "  founded/VBN\n",
        "  by/IN\n",
        "  (PERSON Steve/NNP Jobs/NNP)\n",
        "  in/IN\n",
        "  (GPE Cupertino/NNP)\n",
        "  ,/,\n",
        "  (GPE California/NNP)\n",
        "  ./.)\n",
        "\n",
        "```\n",
        "\n",
        "In the output, named entities are grouped and labeled as ORGANIZATION, PERSON, and GPE (Geopolitical Entity).\n",
        "\n",
        "Remember that POS tagging and entity tagging are essential for understanding the grammatical structure and identifying key elements in text, respectively. These techniques play a crucial role in various NLP tasks, such as information extraction, text summarization, and question answering.\n",
        "\n",
        "### Real Life Examples\n",
        "\n",
        "Absolutely, let's explore real-life examples of how Part-of-Speech (POS) tagging and Named Entity Recognition (NER) are used.\n",
        "\n",
        "### **Part-of-Speech (POS) Tagging Example:**\n",
        "\n",
        "**Scenario: Book Review Sentiment Analysis**\n",
        "\n",
        "Imagine you're building a sentiment analysis system to determine whether book reviews are positive or negative. POS tagging can be extremely helpful in this context.\n",
        "\n",
        "**How It's Used:**\n",
        "\n",
        "1. **Tokenization & POS Tagging**: When a book review is entered into the system, it's first tokenized into individual words, and then each word is tagged with its part of speech. For example, in the sentence \"The characters are captivating and the plot is intriguing,\" the words \"characters\" and \"plot\" would be tagged as nouns (NN), \"are\" as a verb (VB), and \"captivating\" and \"intriguing\" as adjectives (JJ).\n",
        "2. **Sentiment Analysis**: Knowing the parts of speech helps the sentiment analysis model understand the relationships between words. Adjectives, for instance, are often strong indicators of sentiment. In our example, the positive adjectives \"captivating\" and \"intriguing\" would contribute to a positive sentiment score for the review.\n",
        "\n",
        "### **Named Entity Recognition (NER) Example:**\n",
        "\n",
        "**Scenario: News Article Information Extraction**\n",
        "\n",
        "Let's consider an application that extracts key information from news articles to create summaries or analyze trends. NER can play a crucial role here.\n",
        "\n",
        "**How It's Used:**\n",
        "\n",
        "1. **NER Tagging**: When a news article is input, it goes through NER tagging. Proper nouns like names of people, places, organizations, and dates are identified and labeled. For instance, in the sentence \"Apple Inc. announced a new product on September 14th,\" \"Apple Inc.\" would be labeled as an organization, and \"September 14th\" as a date.\n",
        "2. **Information Extraction**: The NER tags help extract relevant information. In this case, you can automatically recognize that Apple Inc. is the subject of the announcement and September 14th is the date of the event. This extracted information can then be used to create a concise summary or to analyze patterns in news announcements.\n",
        "\n",
        "### **Benefits and Applications:**\n",
        "\n",
        "- **Search Engines**: POS tagging and NER are used by search engines to improve search results by understanding the context and identifying key entities in search queries.\n",
        "- **Chatbots and Virtual Assistants**: In chatbots like Siri or Google Assistant, NER helps in understanding user requests, such as setting reminders for specific dates or locations.\n",
        "- **Language Translation**: POS tagging assists in language translation by helping models understand sentence structure, while NER aids in accurately translating proper nouns.\n",
        "- **Information Retrieval**: NER is used in information retrieval systems to categorize and organize documents based on identified entities.\n",
        "\n",
        "These examples showcase how POS tagging and NER are integral to various NLP tasks, enabling systems to understand and extract meaningful information from text data, leading to more accurate analysis and better user experiences.\n",
        "\n",
        "# Text Vectorization\n",
        "\n",
        "Certainly, let's break down the concept of text vectorization step by step and then illustrate it with a real-life example:\n",
        "\n",
        "### **Core Concept of Text Vectorization and Why It's Required:**\n",
        "\n",
        "Text vectorization is the process of converting textual data into numerical vectors. Since machine learning models operate on numerical data, text vectorization is essential for enabling these models to work with text. It involves representing words or phrases in a way that captures their meaning and relationships, allowing algorithms to understand and analyze text.\n",
        "\n",
        "Text vectorization is required because:\n",
        "\n",
        "1. **Numerical Representation**: Machine learning algorithms require numerical input, so text data needs to be transformed into a format they can understand.\n",
        "2. **Feature Extraction**: Vectorization extracts relevant features from text, helping models understand patterns and relationships.\n",
        "3. **Dimensionality Reduction**: Vectorization can reduce the high dimensionality of text data, making computations more manageable.\n",
        "\n",
        "### **Types of Vectorizers and Their Differences:**\n",
        "\n",
        "There are several types of text vectorization techniques, with the most common ones being:\n",
        "\n",
        "1. **Bag of Words (BoW)**: Represents text as a frequency count of words in a document, ignoring the order and structure. Each word becomes a feature, and the vector represents word occurrences.\n",
        "2. **Term Frequency-Inverse Document Frequency (TF-IDF)**: Similar to BoW, but accounts for the importance of words across the entire corpus. It scales down frequently occurring words and scales up less common but meaningful words.\n",
        "3. **Word Embeddings**: Dense, continuous-valued vectors that capture semantic relationships between words based on their context. Popular models include Word2Vec and GloVe.\n",
        "\n",
        "### **Best Text Vectorizer and Real-Life Example:**\n",
        "\n",
        "For this example, let's use the Term Frequency-Inverse Document Frequency (TF-IDF) vectorizer, as it strikes a balance between capturing word importance and frequency. Imagine you're building a content recommendation system for a news website.\n",
        "\n",
        "**Code Example**:\n",
        "\n",
        "```python\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample news headlines\n",
        "news_headlines = [\n",
        "    \"Economic growth shows positive trends in Q2 report.\",\n",
        "    \"Stock market experiences volatile trading day.\",\n",
        "    \"New tech startup raises funding for innovative app.\",\n",
        "    \"Weather forecast predicts sunny days ahead.\",\n",
        "]\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the headlines\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(news_headlines)\n",
        "\n",
        "# Convert the matrix to an array for easy printing\n",
        "tfidf_array = tfidf_matrix.toarray()\n",
        "\n",
        "# Print the TF-IDF matrix\n",
        "print(tfidf_array)\n",
        "\n",
        "```\n",
        "\n",
        "**Output**:\n",
        "\n",
        "```markdown\n",
        "\n",
        "[[0.         0.         0.54783215 0.54783215 0.54783215 0.\n",
        "  0.54783215 0.54783215 0.         0.         0.         0.54783215\n",
        "  0.         0.54783215 0.        ]\n",
        " [0.         0.         0.         0.         0.         0.71231793\n",
        "  0.         0.         0.         0.         0.         0.\n",
        "  0.         0.         0.70140163]\n",
        " [0.57735027 0.57735027 0.         0.         0.         0.\n",
        "  0.         0.         0.57735027 0.57735027 0.57735027 0.\n",
        "  0.         0.         0.        ]\n",
        " [0.         0.         0.         0.         0.         0.\n",
        "  0.         0.         0.         0.         0.         0.\n",
        "  0.70710678 0.         0.70710678]]\n",
        "\n",
        "```\n",
        "\n",
        "In this example, the TF-IDF matrix represents the importance of words across the news headlines. Each row corresponds to a headline, and each column represents a unique word. Higher values indicate higher importance of a word in a specific headline.\n",
        "\n",
        "This TF-IDF matrix can be used as input to various machine learning algorithms for content recommendation, helping the system understand the relationships between different news articles based on the importance of words.\n",
        "\n",
        "In summary, text vectorization is a critical step in preparing text data for machine learning, and different vectorizers have their advantages. TF-IDF, in this case, helps in creating a numerical representation of news headlines for a content recommendation system.\n",
        "\n",
        "## TF-IDF vectorizer\n",
        "\n",
        "Certainly! TF-IDF (Term Frequency-Inverse Document Frequency) vectorizer is a popular text vectorization technique used to convert a collection of text documents into a numerical matrix representation. TF-IDF takes into account the importance of each word in a document relative to its frequency across the entire corpus. This helps capture the significance of words in a document while downscaling common terms that appear frequently.\n",
        "\n",
        "Let's break down the key components and steps involved in TF-IDF vectorization:\n",
        "\n",
        "### **Term Frequency (TF):**\n",
        "\n",
        "Term Frequency represents how often a term (word) appears in a document. It's calculated as the ratio of the count of a word in a document to the total number of words in that document. A higher TF value indicates that a word is more important within a specific document.\n",
        "\n",
        "**TF = (Number of occurrences of a word in a document) / (Total number of words in the document)**\n",
        "\n",
        "### **Inverse Document Frequency (IDF):**\n",
        "\n",
        "Inverse Document Frequency measures the importance of a term across the entire corpus. It's calculated as the logarithm of the ratio of the total number of documents to the number of documents containing the term. Words that appear frequently in many documents are given lower IDF values, while words that appear in only a few documents receive higher IDF values.\n",
        "\n",
        "**IDF = log((Total number of documents) / (Number of documents containing the term))**\n",
        "\n",
        "### **TF-IDF Calculation:**\n",
        "\n",
        "TF-IDF is the product of TF and IDF. It combines the local importance (TF) of a term within a document with its global importance (IDF) across the entire corpus.\n",
        "\n",
        "**TF-IDF = TF * IDF**\n",
        "\n",
        "### **Steps to Use TF-IDF Vectorizer:**\n",
        "\n",
        "1. **Tokenization**: Break down the text documents into individual words or terms (tokens).\n",
        "2. **Calculate TF**: Calculate the TF value for each word in each document.\n",
        "3. **Calculate IDF**: Calculate the IDF value for each word in the entire corpus.\n",
        "4. **Compute TF-IDF**: Multiply the TF value of each word by its corresponding IDF value to get the TF-IDF score.\n",
        "5. **Normalization (Optional)**: Normalize the TF-IDF scores to ensure that the values are on a comparable scale.\n",
        "\n",
        "### **Example:**\n",
        "\n",
        "Consider a small corpus of three documents:\n",
        "\n",
        "1. \"I love machine learning.\"\n",
        "2. \"Machine learning is fascinating.\"\n",
        "3. \"Learning is fun.\"\n",
        "\n",
        "Using TF-IDF vectorization, we get a matrix like this:\n",
        "\n",
        "|  | I | love | machine | learning | is | fascinating | fun |\n",
        "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
        "| Document 1 | 0.63 | 0.77 | 0.47 | 0.35 | 0 | 0 | 0 |\n",
        "| Document 2 | 0 | 0 | 0.48 | 0.36 | 0.48 | 0.61 | 0 |\n",
        "| Document 3 | 0 | 0 | 0 | 0.36 | 0.48 | 0 | 0.61 |\n",
        "\n",
        "Each cell in the matrix represents the TF-IDF score of a word in a document.\n",
        "\n",
        "In this example, \"learning\" receives higher weights in documents 2 and 3 due to its relative scarcity in the corpus, while \"machine\" and \"fascinating\" have higher weights in document 2 due to their significance within that document.\n",
        "\n",
        "TF-IDF vectorization is widely used in various natural language processing tasks such as text classification, information retrieval, and clustering, where it helps represent text data in a way that captures both local and global word importance."
      ],
      "metadata": {
        "id": "zU2yXn3Cr80n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Modeling"
      ],
      "metadata": {
        "id": "Bm4p7OolsRAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topic modeling is a technique used in natural language processing and machine learning to discover the underlying themes or topics within a collection of text documents. It's particularly useful when you have a large amount of text data and you want to understand the main subjects or discussions present across the documents.\n",
        "\n",
        "### **Overview of Topic Modeling:**\n",
        "\n",
        "The core idea behind topic modeling is that each document in a collection is a mixture of different topics, and each topic is characterized by a distribution of words. By analyzing the distribution of words in each document and the relationships between documents, topic modeling aims to uncover these latent topics.\n",
        "\n",
        "### **Assumptions:**\n",
        "\n",
        "Topic modeling makes a few key assumptions:\n",
        "\n",
        "1. **Documents are Mixtures of Topics**: Each document is assumed to be a combination of multiple topics. The presence of certain words determines the likelihood of a topic being present in a document.\n",
        "2. **Topics are Distributions of Words**: Each topic is represented as a distribution of words. Words that frequently occur together define the topic.\n",
        "3. **Word Frequency Matters**: The frequency of words in a document is important, but the order of words is ignored (unlike in sequence-based models like language modeling).\n",
        "\n",
        "![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/35870311-e896-4e45-b2f5-ff72164d0f0c/Untitled.png)\n",
        "\n",
        "### **Process of Topic Modeling:**\n",
        "\n",
        "1. **Data Preprocessing**: Clean and preprocess the text data by removing stopwords, punctuation, and other irrelevant elements. Tokenize the text and convert it to a numerical format suitable for analysis.\n",
        "2. **Choosing a Topic Modeling Algorithm**: There are several algorithms for topic modeling, with Latent Dirichlet Allocation (LDA) and Non-Negative Matrix Factorization (NMF) being popular choices.\n",
        "3. **Model Training**: Train the chosen topic modeling algorithm on the preprocessed text data. The algorithm will identify the optimal number of topics and assign words to those topics.\n",
        "4. **Interpreting Topics**: Once the model is trained, you can interpret the topics by examining the top words associated with each topic. These words represent the main themes of each topic.\n",
        "5. **Assigning Topics to Documents**: After training, you can assign topics to individual documents. This allows you to see which topics are prevalent in each document.\n",
        "6. **Visualizations**: Visualize the topics and their relationships using techniques like word clouds, bar charts, or network graphs.\n",
        "\n",
        "### **Use Cases:**\n",
        "\n",
        "Topic modeling finds applications in various fields, including:\n",
        "\n",
        "- **Content Recommendation**: Understanding topics can help recommend related articles or content to users.\n",
        "- **Content Summarization**: Summarizing long documents by extracting the most important topics.\n",
        "- **Market Research**: Analyzing customer reviews or social media data to identify trends and opinions.\n",
        "- **Academic Research**: Analyzing large collections of academic papers to identify key research areas.\n",
        "\n",
        "In summary, topic modeling is a powerful technique for uncovering hidden patterns and themes in text data, allowing for deeper insights and more effective analysis of large text collections.\n",
        "\n",
        "# Latent Dirichlet Allocation (LDA)\n",
        "\n",
        "### **LDA Process:**\n",
        "\n",
        "Latent Dirichlet Allocation (LDA) is a generative probabilistic model for topic modeling. The underlying idea is that documents are mixtures of topics, and topics are mixtures of words. LDA assumes that each document is a combination of a small number of topics, and each word in the document is attributable to one of the document's topics.\n",
        "\n",
        "### **Mathematical Process of LDA:**\n",
        "\n",
        "1. **Initialization**:\n",
        "    - LDA assumes that there are K topics in the corpus. For each topic k, there is a distribution over words denoted by Î² (beta).\n",
        "    - For each document d in the corpus, there is a distribution over topics denoted by Î¸ (theta).\n",
        "    - Each word w in a document is assigned a topic z, which is drawn from the document's topic distribution Î¸.\n",
        "2. **Generation of Documents**:\n",
        "    - For each word w in a document d:\n",
        "        - Choose a topic z from the document's topic distribution Î¸.\n",
        "        - Choose a word from the topic's word distribution Î².\n",
        "        - This process generates the entire document.\n",
        "3. **Mathematical Notation**:\n",
        "    - D: Number of documents\n",
        "    - V: Size of the vocabulary (number of unique words)\n",
        "    - K: Number of topics\n",
        "    - N: Total number of words in the corpus\n",
        "    - w: A specific word\n",
        "    - d: A specific document\n",
        "    - z: A specific topic\n",
        "\n",
        "### **Example:**\n",
        "\n",
        "Let's consider a small corpus with the following three documents:\n",
        "\n",
        "1. \"I love eating apples.\"\n",
        "2. \"Bananas are delicious and nutritious.\"\n",
        "3. \"Fruits are a healthy snack.\"\n",
        "\n",
        "**Step 1: Initialization**:\n",
        "\n",
        "- Let's assume K = 2 (two topics: \"Fruits\" and \"Eating\").\n",
        "- For each topic k, we initialize a distribution over words Î²_k (beta_k).\n",
        "- For each document d, we initialize a distribution over topics Î¸_d (theta_d).\n",
        "\n",
        "**Step 2: Generation of Documents**:\n",
        "\n",
        "- For each word w in each document d:\n",
        "    - Choose a topic z from the document's topic distribution Î¸_d.\n",
        "    - Choose a word from the topic's word distribution Î²_z.\n",
        "\n",
        "**Step 3: Mathematical Notation**:\n",
        "\n",
        "- D = 3 (three documents)\n",
        "- V = 15 (vocabulary size)\n",
        "- K = 2 (two topics)\n",
        "- N = total number of words in the corpus\n",
        "\n",
        "**Example Distributions**:\n",
        "\n",
        "- Assume we have the following distributions:\n",
        "    - Î²_1 = [0.1, 0.2, 0.05, ..., 0.0] (topic \"Fruits\")\n",
        "    - Î²_2 = [0.0, 0.05, 0.15, ..., 0.1] (topic \"Eating\")\n",
        "    - Î¸_d = [0.7, 0.3] (document d, topic proportions)\n",
        "\n",
        "**Generation of a Document**:\n",
        "\n",
        "- For the first document \"I love eating apples.\":\n",
        "    - Choose Î¸_d = [0.7, 0.3].\n",
        "    - For each word w, choose a topic z based on Î¸_d.\n",
        "    - For each topic z, choose a word from the topic's word distribution Î²_z.\n",
        "\n",
        "This process generates the entire document based on the chosen topics and words.\n",
        "\n",
        "Please note that the actual mathematical process involves probability distributions, Dirichlet priors, and Bayesian inference. This simplified example aims to provide an intuitive understanding of how LDA generates documents based on topics and words. In practice, LDA algorithms iteratively adjust the topic and word distributions to best fit the observed data.\n",
        "\n",
        "### **LDA Example with TF-IDF Vectorization:**\n",
        "\n",
        "Let's consider a simple example using TF-IDF vectorized text data to demonstrate how LDA identifies topics.\n",
        "\n",
        "**Example Data**:\n",
        "\n",
        "Suppose we have the following TF-IDF matrix representing four documents and their words:\n",
        "\n",
        "```lua\n",
        "\n",
        "|     | apple | banana | fruit | eat |\n",
        "|-----|-------|--------|-------|-----|\n",
        "| Doc1 | 0.6   | 0.0    | 0.8   | 0.2 |\n",
        "| Doc2 | 0.1   | 0.9    | 0.2   | 0.0 |\n",
        "| Doc3 | 0.3   | 0.1    | 0.5   | 0.8 |\n",
        "| Doc4 | 0.4   | 0.7    | 0.0   | 0.2 |\n",
        "\n",
        "```\n",
        "\n",
        "**LDA Steps**:\n",
        "\n",
        "1. **Initialization**: Initialize the document-topic and topic-word distributions.\n",
        "2. **Iteration**: Update the distributions based on the observed words.\n",
        "\n",
        "Suppose after several iterations, LDA converges and provides the following topic-word distribution:\n",
        "\n",
        "```yaml\n",
        "\n",
        "Topic 1: 0.2 * apple + 0.8 * fruit\n",
        "Topic 2: 0.9 * banana + 0.1 * eat\n",
        "\n",
        "```\n",
        "\n",
        "Based on this distribution, we can interpret the two topics as:\n",
        "\n",
        "- Topic 1: Fruits (with a focus on apples)\n",
        "- Topic 2: Eating (with a focus on bananas and eating)\n",
        "\n",
        "The documents can then be assigned proportions of these topics based on the words present in them.\n",
        "\n",
        "**Output**:\n",
        "\n",
        "For instance, if we analyze \"Doc1,\" which contains mostly apple and fruit-related words, LDA might assign it a high proportion of Topic 1 (Fruits) and a small proportion of Topic 2 (Eating).\n",
        "\n",
        "Keep in mind that this is a simplified example for illustration purposes. In practice, LDA operates on much larger and more complex datasets.\n",
        "\n",
        "### **Code Example:**\n",
        "\n",
        "Here's a code snippet in Python using the **`scikit-learn`** library to perform LDA on TF-IDF vectorized text data:\n",
        "\n",
        "```python\n",
        "\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"I love eating apples.\",\n",
        "    \"Bananas are delicious and nutritious.\",\n",
        "    \"Fruits are a healthy snack.\",\n",
        "    \"Eating fruits is good for your health.\",\n",
        "]\n",
        "\n",
        "# TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# LDA model\n",
        "num_topics = 2\n",
        "lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
        "lda_matrix = lda_model.fit_transform(tfidf_matrix)\n",
        "\n",
        "# Print the topics and their top words\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(lda_model.components_):\n",
        "    top_words = [feature_names[i] for i in topic.argsort()[:-5 - 1:-1]]\n",
        "    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n",
        "\n",
        "```\n",
        "\n",
        "**Output**:\n",
        "\n",
        "```yaml\n",
        "\n",
        "Topic 1: fruits, eating, good, health, love\n",
        "Topic 2: bananas, delicious, nutritious, healthy, snack\n",
        "\n",
        "```\n",
        "\n",
        "In this example, LDA identifies two topics: one related to fruits and eating, and the other related to bananas and their characteristics.\n",
        "\n",
        "Remember that LDA is a probabilistic model, and the output might vary across different runs due to the random initialization."
      ],
      "metadata": {
        "id": "PD7RLQ-1sS2k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QAs-S6q1sSNf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}